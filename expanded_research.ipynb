{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","import itertools\n","import random\n","import pandas as pd\n","from ceat import ceat_meta\n","import numpy as np\n","from embeddings import generate_embedding, generate_embedding_bulk, visualize_embeddings"]},{"cell_type":"markdown","metadata":{},"source":["Set random seed"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["random.seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["FLOWERS = ['aster', 'clover', 'hyacinth', 'marigold', 'poppy', 'azalea', 'crocus', 'iris', 'orchid', 'rose', 'bluebell', 'daffodil', 'lilac',\n","           'pansy', 'tulip', 'buttercup', 'daisy', 'lily', 'peony', 'violet', 'carnation', 'gladiola', 'magnolia', 'petunia', 'zinnia']\n","\n","INSECTS = ['ant', 'caterpillar', 'flea', 'locust', 'spider', 'bedbug', 'centipede', 'fly', 'maggot', 'tarantula', 'bee', 'cockroach',\n","           'gnat', 'mosquito', 'termite', 'beetle', 'cricket', 'hornet', 'moth', 'wasp', 'blackfly', 'dragonfly', 'horsefly', 'roach', 'weevil']\n","\n","MUSICAL_INSTRUMENTS = ['bagpipe', 'cello', 'guitar', 'lute', 'trombone', 'banjo', 'clarinet', 'harmonica', 'mandolin', 'trumpet', 'bassoon',\n","                       'drum', 'harp', 'oboe', 'tuba', 'bell', 'fiddle', 'harpsichord', 'piano', 'viola', 'bongo', 'flute', 'horn', 'saxophone', 'violin']\n","\n","WEAPONS = ['arrow', 'club', 'gun', 'missile', 'spear', 'axe', 'dagger', 'harpoon', 'pistol', 'sword', 'blade', 'dynamite',\n","           'hatchet', 'rifle', 'tank', 'bomb', 'firearm', 'knife', 'shotgun', 'teargas', 'cannon', 'grenade', 'mace', 'slingshot', 'whip']"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["PLEASANT = ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle',\n","            'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation']\n","\n","UNPLEASANT = ['abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster',\n","              'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison']"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["SENTENCE_TEMPLATES_FLOWERS_INSECTS = [\n","    \"The delicate movements of a(n) {object} evoke feelings of {feeling}\",\n","    \"When I see a(n) {object}, I think of {feeling}\",\n","    \"Watching a(n) {object} fills me with {feeling}\",\n","    \"A {object} in the garden can bring thoughts of {feeling}\",\n","    \"Encountering a(n) {object} makes me feel {feeling}\"\n","]\n","\n","SENTENCE_TEMPLATES_WEAPONS_MUSICAL = [\n","    \"The use of a(n) {object} often leads to {feeling}\",\n","    \"The sound of a(n) {object} brings a sense of {feeling}\",\n","    \"Listening to a(n) {object} evokes feelings of {feeling}\",\n","    \"The misuse of a(n) {object} can be linked to {feeling}\",\n","    \"The careful craftsmanship of a(n) {object} can evoke feelings of {feeling}\",\n","    \"The sight of a sheathed {object} can bring a sense of {feeling}\"\n","]"]},{"cell_type":"markdown","metadata":{},"source":["# Generate sentences and create a dataframe that has all sentences"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def generate_sentences(names, attributes=pd.concat([pd.Series(PLEASANT), pd.Series(UNPLEASANT)]), templates=SENTENCE_TEMPLATES_FLOWERS_INSECTS):\n","    sentences = {}\n","    # Create all possible combinations of attribute, noun and sentence template\n","    triplets = list(itertools.product(attributes, names, templates))\n","    for (a, n, st) in triplets:\n","        sentence = st.format(object=n, feeling=a)\n","        sentences[(a, n, st)] = sentence\n","    return sentences"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["insect_sentence_dict = generate_sentences(names=INSECTS, templates=SENTENCE_TEMPLATES_FLOWERS_INSECTS)\n","flowers_sentence_dict = generate_sentences(names=FLOWERS, templates=SENTENCE_TEMPLATES_FLOWERS_INSECTS)\n","\n","weapons_sentence_dict = generate_sentences(names=MUSICAL_INSTRUMENTS, templates=SENTENCE_TEMPLATES_WEAPONS_MUSICAL)\n","musical_sentence_dict = generate_sentences(names=WEAPONS, templates=SENTENCE_TEMPLATES_WEAPONS_MUSICAL)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["pd_i = pd.DataFrame([(k[0],k[1],k[2],v,\"IN\") for k, v in insect_sentence_dict.items()], columns=[\"attribute\",\"name\",\"sentence_template\",\"sentence\", \"group\"])\n","pd_f = pd.DataFrame([(k[0],k[1],k[2],v,\"FL\") for k, v in flowers_sentence_dict.items()], columns=[\"attribute\",\"name\",\"sentence_template\",\"sentence\", \"group\"])\n","\n","pd_w = pd.DataFrame([(k[0],k[1],k[2],v,\"IN\") for k, v in weapons_sentence_dict.items()], columns=[\"attribute\",\"name\",\"sentence_template\",\"sentence\", \"group\"])\n","pd_m = pd.DataFrame([(k[0],k[1],k[2],v,\"FL\") for k, v in musical_sentence_dict.items()], columns=[\"attribute\",\"name\",\"sentence_template\",\"sentence\", \"group\"])"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["pd_all_if = pd.concat([pd_i,pd_f])\n","pd_all_if = pd_all_if.reset_index()\n","pd_all_if.to_csv(\"dataset_pd_all_insects_flowers.csv\")\n","\n","pd_all_wm = pd.concat([pd_w,pd_m])\n","pd_all_wm = pd_all_wm.reset_index()\n","pd_all_wm.to_csv(\"dataset_pd_all_weapons_musical.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["Print total sentence tokens"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total sentence tokens(make sure this is not too much for OpenAI): 172\n","Total sentence tokens(make sure this is not too much for OpenAI): 240\n"]}],"source":["total_sentence_tokens_if = pd_all_if[\"sentence\"].apply(lambda x: len(x.split(\" \"))).sum()\n","print(f\"Total sentence tokens(make sure this is not too much for OpenAI): {total_sentence_tokens_if}\")\n","assert total_sentence_tokens_if < 1000000 # Make sure we are not over the limit, this is a rough estimate of 13 cents.\n","\n","total_sentence_tokens_wm = pd_all_wm[\"sentence\"].apply(lambda x: len(x.split(\" \"))).sum()\n","print(f\"Total sentence tokens(make sure this is not too much for OpenAI): {total_sentence_tokens_wm}\")\n","assert total_sentence_tokens_wm < 1000000 # Make sure we are not over the limit, this is a rough estimate of 13 cents.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Acquire embeddings for all sentences, code can use some cleaning up"]},{"cell_type":"markdown","metadata":{},"source":["Create dictionary copy of pd_all"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["pd_all_to_dict_if = pd_all_if.to_dict(orient=\"index\")\n","pd_all_to_dict_wm = pd_all_wm.to_dict(orient=\"index\")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["distinct_attributes_if = pd_all_if[\"attribute\"].unique()\n","distinct_groups_if = pd_all_if[\"group\"].unique()\n","\n","distinct_attributes_wm = pd_all_wm[\"attribute\"].unique()\n","distinct_groups_wm = pd_all_wm[\"group\"].unique()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Attribute: caress, Group: IN\n","From cache: 0, To compute: 5\n","WARNING: Computing embeddings for sentences of length 5. This costs $$.\n","Attribute: caress, Group: FL\n","From cache: 0, To compute: 5\n","WARNING: Computing embeddings for sentences of length 5. This costs $$.\n","Attribute: abuse, Group: IN\n","From cache: 0, To compute: 5\n","WARNING: Computing embeddings for sentences of length 5. This costs $$.\n","Attribute: abuse, Group: FL\n","From cache: 0, To compute: 5\n","WARNING: Computing embeddings for sentences of length 5. This costs $$.\n","Attribute: caress, Group: IN\n","From cache: 0, To compute: 6\n","WARNING: Computing embeddings for sentences of length 6. This costs $$.\n","Attribute: caress, Group: FL\n","From cache: 0, To compute: 6\n","WARNING: Computing embeddings for sentences of length 6. This costs $$.\n","Attribute: abuse, Group: IN\n","From cache: 0, To compute: 6\n","WARNING: Computing embeddings for sentences of length 6. This costs $$.\n","Attribute: abuse, Group: FL\n","From cache: 0, To compute: 6\n","WARNING: Computing embeddings for sentences of length 6. This costs $$.\n"]}],"source":["for a in distinct_attributes_if:\n","    for g in distinct_groups_if:\n","        print(f\"Attribute: {a}, Group: {g}\")\n","        data_a_g = pd_all_if[(pd_all_if[\"attribute\"] == a) & (pd_all_if[\"group\"] == g)]\n","        \n","        embedding_str_pairs = generate_embedding_bulk(sentences=data_a_g[\"sentence\"].values,save_to_file=True)\n","        assert len(embedding_str_pairs) == len(data_a_g)\n","        for ((i, row),(sentence, embedding)) in zip(data_a_g.iterrows(),embedding_str_pairs):\n","            assert row[\"sentence\"] == sentence              \n","\n","            # Store array in pd_all_to_dict\n","            pd_all_to_dict_if[i][\"embedding\"] = np.array(embedding)\n","\n","\n","for a in distinct_attributes_wm:\n","    for g in distinct_groups_wm:\n","        print(f\"Attribute: {a}, Group: {g}\")\n","        data_a_g = pd_all_wm[(pd_all_wm[\"attribute\"] == a) & (pd_all_wm[\"group\"] == g)]\n","        \n","        embedding_str_pairs = generate_embedding_bulk(sentences=data_a_g[\"sentence\"].values,save_to_file=True)\n","        assert len(embedding_str_pairs) == len(data_a_g)\n","        for ((i, row),(sentence, embedding)) in zip(data_a_g.iterrows(),embedding_str_pairs):\n","            assert row[\"sentence\"] == sentence              \n","\n","            # Store array in pd_all_to_dict\n","            pd_all_to_dict_wm[i][\"embedding\"] = np.array(embedding)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tao>0\n","(-0.1421465624475714, 1.0)\n","_, p-value\n"]}],"source":["sent2emb = defaultdict(list)\n","for emb in pd_all_to_dict_wm.values():\n","    sent2emb[emb[\"name\"]].append(emb[\"embedding\"])\n","    sent2emb[emb[\"attribute\"]].append(emb[\"embedding\"])\n","print(ceat_meta([[WEAPONS,MUSICAL_INSTRUMENTS,PLEASANT,UNPLEASANT]],sent2emb,N = 1000))\n","print(\"_, p-value\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tao>0\n","(0.016405586602217595, 0.36891452786261547)\n","_, p-value\n"]}],"source":["sent2emb = defaultdict(list)\n","for emb in pd_all_to_dict_if.values():\n","    sent2emb[emb[\"name\"]].append(emb[\"embedding\"])\n","    sent2emb[emb[\"attribute\"]].append(emb[\"embedding\"])\n","print(ceat_meta([[INSECTS,FLOWERS,PLEASANT,UNPLEASANT]],sent2emb,N = 1000))\n","print(\"_, p-value\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":2}
